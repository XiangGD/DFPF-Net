{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f746ea0d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ccce99b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weights = np.load('./models/vit_checkpoint/imagenet21k/R50+ViT-B_16.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d35699",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load('./models/FU_PS/FU_train_R50-ViT-B_16_skip3_epo70_bs21_Adam_Customized_lr0.0001_256/epo34.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc1c7c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.transformer.embeddings.position_embeddings:torch.Size([1, 256, 768])\n",
      "module.transformer.embeddings.fpn.P4.weight:torch.Size([1024, 2048, 1, 1])\n",
      "module.transformer.embeddings.fpn.P4.bias:torch.Size([1024])\n",
      "module.transformer.embeddings.fpn.P3_1.weight:torch.Size([512, 1024, 1, 1])\n",
      "module.transformer.embeddings.fpn.P3_1.bias:torch.Size([512])\n",
      "module.transformer.embeddings.fpn.P3_2.weight:torch.Size([512, 1024, 3, 3])\n",
      "module.transformer.embeddings.fpn.P3_2.bias:torch.Size([512])\n",
      "module.transformer.embeddings.fpn.P2_1.weight:torch.Size([128, 512, 1, 1])\n",
      "module.transformer.embeddings.fpn.P2_1.bias:torch.Size([128])\n",
      "module.transformer.embeddings.fpn.P2_2.weight:torch.Size([256, 512, 3, 3])\n",
      "module.transformer.embeddings.fpn.P2_2.bias:torch.Size([256])\n",
      "module.transformer.embeddings.fpn.P1.weight:torch.Size([64, 128, 3, 3])\n",
      "module.transformer.embeddings.fpn.P1.bias:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.root.conv.weight:torch.Size([64, 3, 7, 7])\n",
      "module.transformer.embeddings.hybrid_model.root.gn.weight:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.root.gn.bias:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit1.gn1.weight:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit1.gn1.bias:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit1.conv1.weight:torch.Size([64, 64, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit1.gn2.weight:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit1.gn2.bias:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit1.conv2.weight:torch.Size([64, 64, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit1.gn3.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit1.gn3.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit1.conv3.weight:torch.Size([256, 64, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit1.downsample.weight:torch.Size([256, 64, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit1.gn_proj.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit1.gn_proj.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit2.gn1.weight:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit2.gn1.bias:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit2.conv1.weight:torch.Size([64, 256, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit2.gn2.weight:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit2.gn2.bias:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit2.conv2.weight:torch.Size([64, 64, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit2.gn3.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit2.gn3.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit2.conv3.weight:torch.Size([256, 64, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit3.gn1.weight:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit3.gn1.bias:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit3.conv1.weight:torch.Size([64, 256, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit3.gn2.weight:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit3.gn2.bias:torch.Size([64])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit3.conv2.weight:torch.Size([64, 64, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit3.gn3.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit3.gn3.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block1.unit3.conv3.weight:torch.Size([256, 64, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit1.gn1.weight:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit1.gn1.bias:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit1.conv1.weight:torch.Size([128, 256, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit1.gn2.weight:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit1.gn2.bias:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit1.conv2.weight:torch.Size([128, 128, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit1.gn3.weight:torch.Size([512])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit1.gn3.bias:torch.Size([512])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit1.conv3.weight:torch.Size([512, 128, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit1.downsample.weight:torch.Size([512, 256, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit1.gn_proj.weight:torch.Size([512])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit1.gn_proj.bias:torch.Size([512])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit2.gn1.weight:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit2.gn1.bias:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit2.conv1.weight:torch.Size([128, 512, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit2.gn2.weight:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit2.gn2.bias:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit2.conv2.weight:torch.Size([128, 128, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit2.gn3.weight:torch.Size([512])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit2.gn3.bias:torch.Size([512])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit2.conv3.weight:torch.Size([512, 128, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit3.gn1.weight:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit3.gn1.bias:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit3.conv1.weight:torch.Size([128, 512, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit3.gn2.weight:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit3.gn2.bias:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit3.conv2.weight:torch.Size([128, 128, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit3.gn3.weight:torch.Size([512])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit3.gn3.bias:torch.Size([512])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit3.conv3.weight:torch.Size([512, 128, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit4.gn1.weight:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit4.gn1.bias:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit4.conv1.weight:torch.Size([128, 512, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit4.gn2.weight:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit4.gn2.bias:torch.Size([128])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit4.conv2.weight:torch.Size([128, 128, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit4.gn3.weight:torch.Size([512])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit4.gn3.bias:torch.Size([512])\n",
      "module.transformer.embeddings.hybrid_model.body.block2.unit4.conv3.weight:torch.Size([512, 128, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit1.gn1.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit1.gn1.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit1.conv1.weight:torch.Size([256, 512, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit1.gn2.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit1.gn2.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit1.conv2.weight:torch.Size([256, 256, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit1.gn3.weight:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit1.gn3.bias:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit1.conv3.weight:torch.Size([1024, 256, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit1.downsample.weight:torch.Size([1024, 512, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit1.gn_proj.weight:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit1.gn_proj.bias:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit2.gn1.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit2.gn1.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit2.conv1.weight:torch.Size([256, 1024, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit2.gn2.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit2.gn2.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit2.conv2.weight:torch.Size([256, 256, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit2.gn3.weight:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit2.gn3.bias:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit2.conv3.weight:torch.Size([1024, 256, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit3.gn1.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit3.gn1.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit3.conv1.weight:torch.Size([256, 1024, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit3.gn2.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit3.gn2.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit3.conv2.weight:torch.Size([256, 256, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit3.gn3.weight:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit3.gn3.bias:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit3.conv3.weight:torch.Size([1024, 256, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit4.gn1.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit4.gn1.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit4.conv1.weight:torch.Size([256, 1024, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit4.gn2.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit4.gn2.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit4.conv2.weight:torch.Size([256, 256, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit4.gn3.weight:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit4.gn3.bias:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit4.conv3.weight:torch.Size([1024, 256, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit5.gn1.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit5.gn1.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit5.conv1.weight:torch.Size([256, 1024, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit5.gn2.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit5.gn2.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit5.conv2.weight:torch.Size([256, 256, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit5.gn3.weight:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit5.gn3.bias:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit5.conv3.weight:torch.Size([1024, 256, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit6.gn1.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit6.gn1.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit6.conv1.weight:torch.Size([256, 1024, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit6.gn2.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit6.gn2.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit6.conv2.weight:torch.Size([256, 256, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit6.gn3.weight:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit6.gn3.bias:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit6.conv3.weight:torch.Size([1024, 256, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit7.gn1.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit7.gn1.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit7.conv1.weight:torch.Size([256, 1024, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit7.gn2.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit7.gn2.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit7.conv2.weight:torch.Size([256, 256, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit7.gn3.weight:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit7.gn3.bias:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit7.conv3.weight:torch.Size([1024, 256, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit8.gn1.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit8.gn1.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit8.conv1.weight:torch.Size([256, 1024, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit8.gn2.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit8.gn2.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit8.conv2.weight:torch.Size([256, 256, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit8.gn3.weight:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit8.gn3.bias:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit8.conv3.weight:torch.Size([1024, 256, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit9.gn1.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit9.gn1.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit9.conv1.weight:torch.Size([256, 1024, 1, 1])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit9.gn2.weight:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit9.gn2.bias:torch.Size([256])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit9.conv2.weight:torch.Size([256, 256, 3, 3])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit9.gn3.weight:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit9.gn3.bias:torch.Size([1024])\n",
      "module.transformer.embeddings.hybrid_model.body.block3.unit9.conv3.weight:torch.Size([1024, 256, 1, 1])\n",
      "module.transformer.embeddings.patch_embeddings.weight:torch.Size([768, 1024, 1, 1])\n",
      "module.transformer.embeddings.patch_embeddings.bias:torch.Size([768])\n",
      "module.transformer.embeddings.constr_conv.kernel_r:torch.Size([6, 1, 5, 5])\n",
      "module.transformer.embeddings.constr_conv.kernel_g:torch.Size([6, 1, 5, 5])\n",
      "module.transformer.embeddings.constr_conv.kernel_b:torch.Size([6, 1, 5, 5])\n",
      "module.transformer.embeddings.constr_conv.conv1x1_r.weight:torch.Size([1, 6, 1, 1])\n",
      "module.transformer.embeddings.constr_conv.conv1x1_r.bias:torch.Size([1])\n",
      "module.transformer.embeddings.constr_conv.conv1x1_g.weight:torch.Size([1, 6, 1, 1])\n",
      "module.transformer.embeddings.constr_conv.conv1x1_g.bias:torch.Size([1])\n",
      "module.transformer.embeddings.constr_conv.conv1x1_b.weight:torch.Size([1, 6, 1, 1])\n",
      "module.transformer.embeddings.constr_conv.conv1x1_b.bias:torch.Size([1])\n",
      "module.transformer.encoder.layer.0.attention_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.0.attention_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.0.ffn_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.0.ffn_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.0.ffn.fc1.weight:torch.Size([3072, 768])\n",
      "module.transformer.encoder.layer.0.ffn.fc1.bias:torch.Size([3072])\n",
      "module.transformer.encoder.layer.0.ffn.fc2.weight:torch.Size([768, 3072])\n",
      "module.transformer.encoder.layer.0.ffn.fc2.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.0.attn.query.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.0.attn.query.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.0.attn.key.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.0.attn.key.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.0.attn.value.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.0.attn.value.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.0.attn.out.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.0.attn.out.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.1.attention_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.1.attention_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.1.ffn_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.1.ffn_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.1.ffn.fc1.weight:torch.Size([3072, 768])\n",
      "module.transformer.encoder.layer.1.ffn.fc1.bias:torch.Size([3072])\n",
      "module.transformer.encoder.layer.1.ffn.fc2.weight:torch.Size([768, 3072])\n",
      "module.transformer.encoder.layer.1.ffn.fc2.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.1.attn.query.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.1.attn.query.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.1.attn.key.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.1.attn.key.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.1.attn.value.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.1.attn.value.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.1.attn.out.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.1.attn.out.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.2.attention_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.2.attention_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.2.ffn_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.2.ffn_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.2.ffn.fc1.weight:torch.Size([3072, 768])\n",
      "module.transformer.encoder.layer.2.ffn.fc1.bias:torch.Size([3072])\n",
      "module.transformer.encoder.layer.2.ffn.fc2.weight:torch.Size([768, 3072])\n",
      "module.transformer.encoder.layer.2.ffn.fc2.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.2.attn.query.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.2.attn.query.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.2.attn.key.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.2.attn.key.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.2.attn.value.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.2.attn.value.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.2.attn.out.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.2.attn.out.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.3.attention_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.3.attention_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.3.ffn_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.3.ffn_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.3.ffn.fc1.weight:torch.Size([3072, 768])\n",
      "module.transformer.encoder.layer.3.ffn.fc1.bias:torch.Size([3072])\n",
      "module.transformer.encoder.layer.3.ffn.fc2.weight:torch.Size([768, 3072])\n",
      "module.transformer.encoder.layer.3.ffn.fc2.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.3.attn.query.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.3.attn.query.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.3.attn.key.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.3.attn.key.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.3.attn.value.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.3.attn.value.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.3.attn.out.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.3.attn.out.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.4.attention_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.4.attention_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.4.ffn_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.4.ffn_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.4.ffn.fc1.weight:torch.Size([3072, 768])\n",
      "module.transformer.encoder.layer.4.ffn.fc1.bias:torch.Size([3072])\n",
      "module.transformer.encoder.layer.4.ffn.fc2.weight:torch.Size([768, 3072])\n",
      "module.transformer.encoder.layer.4.ffn.fc2.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.4.attn.query.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.4.attn.query.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.4.attn.key.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.4.attn.key.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.4.attn.value.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.4.attn.value.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.4.attn.out.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.4.attn.out.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.5.attention_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.5.attention_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.5.ffn_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.5.ffn_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.5.ffn.fc1.weight:torch.Size([3072, 768])\n",
      "module.transformer.encoder.layer.5.ffn.fc1.bias:torch.Size([3072])\n",
      "module.transformer.encoder.layer.5.ffn.fc2.weight:torch.Size([768, 3072])\n",
      "module.transformer.encoder.layer.5.ffn.fc2.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.5.attn.query.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.5.attn.query.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.5.attn.key.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.5.attn.key.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.5.attn.value.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.5.attn.value.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.5.attn.out.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.5.attn.out.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.6.attention_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.6.attention_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.6.ffn_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.6.ffn_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.6.ffn.fc1.weight:torch.Size([3072, 768])\n",
      "module.transformer.encoder.layer.6.ffn.fc1.bias:torch.Size([3072])\n",
      "module.transformer.encoder.layer.6.ffn.fc2.weight:torch.Size([768, 3072])\n",
      "module.transformer.encoder.layer.6.ffn.fc2.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.6.attn.query.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.6.attn.query.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.6.attn.key.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.6.attn.key.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.6.attn.value.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.6.attn.value.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.6.attn.out.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.6.attn.out.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.7.attention_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.7.attention_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.7.ffn_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.7.ffn_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.7.ffn.fc1.weight:torch.Size([3072, 768])\n",
      "module.transformer.encoder.layer.7.ffn.fc1.bias:torch.Size([3072])\n",
      "module.transformer.encoder.layer.7.ffn.fc2.weight:torch.Size([768, 3072])\n",
      "module.transformer.encoder.layer.7.ffn.fc2.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.7.attn.query.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.7.attn.query.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.7.attn.key.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.7.attn.key.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.7.attn.value.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.7.attn.value.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.7.attn.out.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.7.attn.out.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.8.attention_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.8.attention_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.8.ffn_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.8.ffn_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.8.ffn.fc1.weight:torch.Size([3072, 768])\n",
      "module.transformer.encoder.layer.8.ffn.fc1.bias:torch.Size([3072])\n",
      "module.transformer.encoder.layer.8.ffn.fc2.weight:torch.Size([768, 3072])\n",
      "module.transformer.encoder.layer.8.ffn.fc2.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.8.attn.query.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.8.attn.query.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.8.attn.key.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.8.attn.key.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.8.attn.value.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.8.attn.value.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.8.attn.out.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.8.attn.out.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.9.attention_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.9.attention_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.9.ffn_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.9.ffn_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.9.ffn.fc1.weight:torch.Size([3072, 768])\n",
      "module.transformer.encoder.layer.9.ffn.fc1.bias:torch.Size([3072])\n",
      "module.transformer.encoder.layer.9.ffn.fc2.weight:torch.Size([768, 3072])\n",
      "module.transformer.encoder.layer.9.ffn.fc2.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.9.attn.query.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.9.attn.query.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.9.attn.key.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.9.attn.key.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.9.attn.value.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.9.attn.value.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.9.attn.out.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.9.attn.out.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.10.attention_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.10.attention_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.10.ffn_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.10.ffn_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.10.ffn.fc1.weight:torch.Size([3072, 768])\n",
      "module.transformer.encoder.layer.10.ffn.fc1.bias:torch.Size([3072])\n",
      "module.transformer.encoder.layer.10.ffn.fc2.weight:torch.Size([768, 3072])\n",
      "module.transformer.encoder.layer.10.ffn.fc2.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.10.attn.query.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.10.attn.query.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.10.attn.key.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.10.attn.key.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.10.attn.value.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.10.attn.value.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.10.attn.out.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.10.attn.out.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.11.attention_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.11.attention_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.11.ffn_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.layer.11.ffn_norm.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.11.ffn.fc1.weight:torch.Size([3072, 768])\n",
      "module.transformer.encoder.layer.11.ffn.fc1.bias:torch.Size([3072])\n",
      "module.transformer.encoder.layer.11.ffn.fc2.weight:torch.Size([768, 3072])\n",
      "module.transformer.encoder.layer.11.ffn.fc2.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.11.attn.query.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.11.attn.query.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.11.attn.key.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.11.attn.key.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.11.attn.value.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.11.attn.value.bias:torch.Size([768])\n",
      "module.transformer.encoder.layer.11.attn.out.weight:torch.Size([768, 768])\n",
      "module.transformer.encoder.layer.11.attn.out.bias:torch.Size([768])\n",
      "module.transformer.encoder.encoder_norm.weight:torch.Size([768])\n",
      "module.transformer.encoder.encoder_norm.bias:torch.Size([768])\n",
      "module.decoder.conv_more.0.weight:torch.Size([512, 768, 3, 3])\n",
      "module.decoder.conv_more.1.weight:torch.Size([512])\n",
      "module.decoder.conv_more.1.bias:torch.Size([512])\n",
      "module.decoder.conv_more.1.running_mean:torch.Size([512])\n",
      "module.decoder.conv_more.1.running_var:torch.Size([512])\n",
      "module.decoder.conv_more.1.num_batches_tracked:torch.Size([])\n",
      "module.decoder.blocks.0.conv1.0.weight:torch.Size([256, 1024, 3, 3])\n",
      "module.decoder.blocks.0.conv1.1.weight:torch.Size([256])\n",
      "module.decoder.blocks.0.conv1.1.bias:torch.Size([256])\n",
      "module.decoder.blocks.0.conv1.1.running_mean:torch.Size([256])\n",
      "module.decoder.blocks.0.conv1.1.running_var:torch.Size([256])\n",
      "module.decoder.blocks.0.conv1.1.num_batches_tracked:torch.Size([])\n",
      "module.decoder.blocks.1.conv1.0.weight:torch.Size([128, 512, 3, 3])\n",
      "module.decoder.blocks.1.conv1.1.weight:torch.Size([128])\n",
      "module.decoder.blocks.1.conv1.1.bias:torch.Size([128])\n",
      "module.decoder.blocks.1.conv1.1.running_mean:torch.Size([128])\n",
      "module.decoder.blocks.1.conv1.1.running_var:torch.Size([128])\n",
      "module.decoder.blocks.1.conv1.1.num_batches_tracked:torch.Size([])\n",
      "module.decoder.blocks.2.conv1.0.weight:torch.Size([64, 192, 3, 3])\n",
      "module.decoder.blocks.2.conv1.1.weight:torch.Size([64])\n",
      "module.decoder.blocks.2.conv1.1.bias:torch.Size([64])\n",
      "module.decoder.blocks.2.conv1.1.running_mean:torch.Size([64])\n",
      "module.decoder.blocks.2.conv1.1.running_var:torch.Size([64])\n",
      "module.decoder.blocks.2.conv1.1.num_batches_tracked:torch.Size([])\n",
      "module.decoder.blocks.3.conv1.0.weight:torch.Size([16, 64, 3, 3])\n",
      "module.decoder.blocks.3.conv1.1.weight:torch.Size([16])\n",
      "module.decoder.blocks.3.conv1.1.bias:torch.Size([16])\n",
      "module.decoder.blocks.3.conv1.1.running_mean:torch.Size([16])\n",
      "module.decoder.blocks.3.conv1.1.running_var:torch.Size([16])\n",
      "module.decoder.blocks.3.conv1.1.num_batches_tracked:torch.Size([])\n",
      "module.segmentation_head.0.weight:torch.Size([1, 16, 3, 3])\n",
      "module.segmentation_head.0.bias:torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for k,v in weights['model'].items():\n",
    "    print('{}:{}'.format(k, v.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3a8f7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 12, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = weights['Transformer/encoderblock_0/MultiHeadDotProductAttention_1/key/kernel']\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "752de85e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4206b34f3931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mconfig_vit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_skip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mconfig_vit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvit_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'R50'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#test baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mconfig_vit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mViT_seg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_vit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_vit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from networks.vit_seg_modeling import ForensicTransformer as ViT_seg\n",
    "from networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\n",
    "from thop import profile, clever_format\n",
    "config_vit = CONFIGS_ViT_seg['R50-ViT-B_16']\n",
    "config_vit.n_classes = 1\n",
    "config_vit.n_skip = 3\n",
    "config_vit.patches.size = (16, 16)\n",
    "config_vit.patches.grid = (16, 16)\n",
    "net = ViT_seg(config_vit, img_size=256, num_classes=config_vit.n_classes).to(device)\n",
    "net.to(device)\n",
    "if args.num_gpu > 1:\n",
    "    net = nn.DataParallel(net)\n",
    "snapshot = './models/FU_PS/FU_fine_tuned_R50-ViT-B_16_skip3_epo70_bs21_Adam_Customized_lr0.0001_256/epo63.pth'\n",
    "net.load_state_dict(torch.load(snapshot)['model'])\n",
    "inputs = torch.randn(1,3,224,224).cuda()\n",
    "macs, params = profile(net, inputs=(inputs,))\n",
    "macs, params = clever_format([macs, params],'%.3f')\n",
    "print(macs, params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee51310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
